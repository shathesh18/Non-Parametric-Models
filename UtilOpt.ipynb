{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    1-D Expected Improvement\n",
    "    Taken from Professor Varun Notebook\n",
    "'''\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize, fmin\n",
    "import GPy\n",
    "import numpy as np\n",
    "\n",
    "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01, mpi=False):\n",
    "    '''\n",
    "    Computes the EI at points X based on existing samples X_sample\n",
    "    and Y_sample using a Gaussian process surrogate model.\n",
    "    \n",
    "    Args:\n",
    "        X: Points at which EI shall be computed (m x d).\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "        xi: Exploitation-exploration trade-off parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Expected improvements at points X.\n",
    "    '''\n",
    "    mu, sigma = gpr.predict(X)\n",
    "    mu_sample = gpr.predict(X_sample)\n",
    "\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    \n",
    "    # Needed for noise-based model,\n",
    "    # otherwise use np.max(Y_sample).\n",
    "    mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "    if(mpi):\n",
    "        return norm.cdf(Z)\n",
    "    return ei.flatten()\n",
    "\n",
    "def propose_location(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=25):\n",
    "    '''\n",
    "    Proposes the next sampling point by optimizing the acquisition function.\n",
    "    \n",
    "    Args:\n",
    "        acquisition: Acquisition function.\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "\n",
    "    Returns:\n",
    "        Location of the acquisition function maximum.\n",
    "    '''\n",
    "    dim = X_sample.shape[1]\n",
    "    min_val = 1\n",
    "    min_x = None\n",
    "    \n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr)\n",
    "    \n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    for x0 in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim)):\n",
    "        res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B')        \n",
    "        if res.fun < min_val:\n",
    "            min_val = res.fun[0]\n",
    "            min_x = res.x           \n",
    "            \n",
    "    return min_x.reshape(-1, 1)\n",
    "\n",
    "def min_obj(X):\n",
    "    # Minimization objective is the negative acquisition function\n",
    "    return -expected_improvement(X.reshape(-1, dim), X_sample, Y_sample, gpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    2-D Expected Improvement\n",
    "'''\n",
    "def expected_improvement_2d(X, X_sample, Y_sample, gpr, xi=0.01, mpi=False,plot=False):\n",
    "    '''\n",
    "    Computes the EI at points X based on existing samples X_sample\n",
    "    and Y_sample using a Gaussian process surrogate model.\n",
    "    \n",
    "    Args:\n",
    "        X: Points at which EI shall be computed (m x d).\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "        xi: Exploitation-exploration trade-off parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Expected improvements at points X.\n",
    "    '''\n",
    "    #print(plot)\n",
    "    if plot:\n",
    "        x1=X[:,0]\n",
    "        x2=X[:,1]\n",
    "        X,Y=grid=np.meshgrid(x1,x2)\n",
    "        X=np.vstack([X.flatten(),Y.flatten()]).T\n",
    "        #print(X.shape)\n",
    "    mu, sigma = gpr.predict(X)\n",
    "    mu_sample = gpr.predict(X_sample)\n",
    "\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    \n",
    "    # Needed for noise-based model,\n",
    "    # otherwise use np.max(Y_sample).\n",
    "    mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "    if(mpi):\n",
    "        return norm.cdf(Z)\n",
    "    return ei.flatten()\n",
    "\n",
    "def propose_location_2d(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=25):\n",
    "    '''\n",
    "    Proposes the next sampling point by optimizing the acquisition function.\n",
    "    \n",
    "    Args:\n",
    "        acquisition: Acquisition function.\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "\n",
    "    Returns:\n",
    "        Location of the acquisition function maximum.\n",
    "    '''\n",
    "    dim = X_sample.shape[1]\n",
    "    min_val = 1\n",
    "    min_x = None\n",
    "    \n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr)\n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    for (x0,x1) in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim)):\n",
    "    #for (x0,x1) in np.random.uniform(bounds[0], bounds[1], size=(n_restarts, dim)):\n",
    "        #res = minimize(min_obj, x0=[x0,x1], bounds=np.vstack((bounds, bounds)), method='L-BFGS-B')\n",
    "        #res = minimize(min_obj, x0=[x0,x1], bounds=((-1, 1), (-1, 1)), method='TNC')\n",
    "        res = minimize(min_obj, x0=[x0,x1], \\\n",
    "                       bounds=bounds, method='L-BFGS-B')\n",
    "        if res.fun < min_val:\n",
    "            min_val = res.fun[0]\n",
    "            min_x = res.x           \n",
    "            \n",
    "    return min_x.reshape(-1, 1)\n",
    "\n",
    "def min_obj(X):\n",
    "    # Minimization objective is the negative acquisition function\n",
    "    return -expected_improvement_2d(X.reshape(-1, dim), X_sample, Y_sample, gpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Sampling a Gaussian from the posterior mu, var\n",
    "    fit from the augmented data fit posterior \n",
    "'''\n",
    "def sample_gaussian(mu,var):\n",
    "    sample=[]\n",
    "    for i,mean in enumerate(mu):\n",
    "        #sample.append(np.random.multivariate_normal(np.asarray(mean),np.asarray(var[i]).reshape(-1,1))[0])\n",
    "        sam=np.random.multivariate_normal(np.asarray(mean),np.asarray(var[i]).reshape(-1,1))[0]\n",
    "        if var[i]>50:\n",
    "            sam = mean + (var[i]/2)*(np.random.choice(np.array([-1,1])))*np.random.sample()\n",
    "        sample.append(sam)\n",
    "    return np.array(sample)\n",
    "\n",
    "\n",
    "def thompson_sampling(n_iter, X, X_init, Y_init, f, kernel, \\\n",
    "             mf=None, lr=0.4,plotPlots=False):\n",
    "    '''\n",
    "        Thompson Sampling\n",
    "    '''\n",
    "    X_sample = X_init\n",
    "    Y_sample = Y_init \n",
    "    for i in range(n_iter):\n",
    "        if(mf):\n",
    "            gpr = GPy.models.GPRegression(X_sample,Y_sample,kernel=kernel, \\\n",
    "                                          noise_var=.3, mean_function=mf)\n",
    "        else:\n",
    "            gpr = GPy.models.GPRegression(X_sample,Y_sample,kernel=kernel)\n",
    "\n",
    "        mu, var = gpr.predict(X)\n",
    "        cov = gpr.kern.K(X)\n",
    "        thompson_sample = sample_gaussian(mu, var)\n",
    "        Y_eval_index = np.argmax(thompson_sample)\n",
    "        X_next = X[Y_eval_index]\n",
    "        Y_next = f(gamma=X_next, live=False, lr=lr, best_samples=Y_p.flatten()) + np.random.rand() * -100 * (np.random.choice(np.array([-1,1])))\n",
    "        \n",
    "        if(plotPlots):\n",
    "            plt.subplot(n_iter, 1, i + 1)\n",
    "            plt.fill_between(X.ravel(), (mu - var).ravel(), (mu + var).ravel(), alpha=0.2)\n",
    "            plt.plot(X.ravel(), mu.ravel(), label='Mean')\n",
    "            plt.plot(X.ravel(), thompson_sample, lw=2, ls='--', label='Thompson Sample')\n",
    "            plt.plot(X_sample, Y_sample, 'kx', mew=1.8, label='Acquisition Fn - Next Sample')\n",
    "            #plt.xlabel('Gamma')\n",
    "            #plt.ylabel('Taxi Reward')\n",
    "            plt.axvline(x=X_next, ls='--', c='k', lw=1, label='Next sampling location')\n",
    "            plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "        \n",
    "        X_sample = np.vstack((X_sample, X_next))\n",
    "        Y_sample = np.vstack((Y_sample, Y_next))\n",
    "    return X_sample, Y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Plot methods\n",
    "    * 1D case - Professor Varun's plot function from Notebooks\n",
    "    * 2D case\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_contour(gpr, X, X_sample, Y_sample, X_next=np.array([]), noisy_label='Noisy', show_legend=False):\n",
    "    #print('x',X[0].shape)\n",
    "    x1,x2=X\n",
    "    X,Y=grid=np.meshgrid(x1,x2)\n",
    "    x_test=np.vstack([X.flatten(),Y.flatten()]).T\n",
    "    #print('x_test',x_test.shape)\n",
    "    mu,var=gpr.predict(x_test)\n",
    "    means=mu\n",
    "    plot_means=means.reshape(X[0].shape[0],X[0].shape[0])\n",
    "    #print('x_sample ',X_sample.shape, x_test.shape)\n",
    "    plt.plot(X_sample[:, 0], X_sample[:, 1], 'kx', mew=3, label='Noisy samples')\n",
    "    if(X_next.size > 0):\n",
    "        plt.plot(X_next.T[:, 0], X_next.T[:, 1], 'co', mew=3, label='Acquisition Fn - Next Sample')\n",
    "    #CS = plt.contour(X, Y, plot_means,levels=200, linewidths=0.5, colors='k')\n",
    "    cntr1 = plt.contourf(X, Y, plot_means, levels=700, cmap=\"RdBu_r\")\n",
    "    #plt.clabel(CS, inline=1, fontsize=10)\n",
    "    if(show_legend):\n",
    "        plt.legend()\n",
    "\n",
    "    \n",
    "def plot_contour_var(gpr, X, X_sample, Y_sample,  X_next=np.array([]), noisy_label='Noisy', showLegend=False):\n",
    "    x1,x2=X\n",
    "    X,Y=grid=np.meshgrid(x1,x2)\n",
    "    x_test=np.vstack([X.flatten(),Y.flatten()]).T\n",
    "    mu,var=gpr.predict(x_test)\n",
    "    means=var\n",
    "    plot_means=means.reshape(X[0].shape[0],X[0].shape[0])\n",
    "    #print('x_sample ',X_sample.shape, x_test.shape)\n",
    "    plt.plot(X_sample[:, 0], X_sample[:, 1], 'kx', mew=1, label='Samples')\n",
    "    #CS = plt.contour(X, Y, plot_means,levels=200, linewidths=0.5, colors='k')\n",
    "    cntr1 = plt.contourf(X, Y, plot_means, levels=700, cmap=\"RdBu_r\")\n",
    "    if(X_next.size > 0):\n",
    "        plt.plot(X_next.T[:, 0], X_next.T[:, 1], 'co', mew=3, label='Acquisition Fn - Next Sample')\n",
    "    #plt.clabel(CS, inline=1, fontsize=10)\n",
    "    if(showLegend):\n",
    "        plt.legend()\n",
    "        #plt.colorbar(cntr1)#, ax=ax)\n",
    "    \n",
    "def plot_app(gpr, X, X_sample, Y_sample, X_next=None, noisy_label='Noisy', show_legend=False):\n",
    "    mu, std = gpr.predict(X)\n",
    "    #print(X.ravel())\n",
    "    #print(mu.ravel())\n",
    "    #print(std.ravel())\n",
    "    plt.fill_between(X.ravel(), \n",
    "                     mu.ravel() + 1.96 * std.ravel(), \n",
    "                     mu.ravel() - 1.96 * std.ravel(), \n",
    "                     alpha=0.1) \n",
    "    #plt.plot(X, Y, 'y--', lw=1, label='Noise-free objective')\n",
    "    plt.plot(X, mu, 'b-', lw=1, label='Mean function')\n",
    "    plt.plot(X_sample, Y_sample, 'kx', mew=1, label=noisy_label+' samples')\n",
    "    if X_next:\n",
    "        plt.axvline(x=X_next, ls='--', c='k', lw=1)\n",
    "    if show_legend:\n",
    "        plt.legend()\n",
    "\n",
    "def plot_approximation(gpr, X, X_sample, Y_sample, X_next=None, show_legend=False):\n",
    "    mu, std = gpr.predict(X)\n",
    "    plt.fill_between(X.ravel(), \n",
    "                     mu.ravel() + 1.96 * std, \n",
    "                     mu.ravel() - 1.96 * std, \n",
    "                     alpha=0.1) \n",
    "    #plt.plot(X, Y, 'y--', lw=1, label='Noise-free objective')\n",
    "    plt.plot(X, mu, 'b-', lw=1, label='Surrogate function')\n",
    "    plt.plot(X_sample, Y_sample, 'kx', mew=1, label='Noisy samples')\n",
    "    if X_next:\n",
    "        plt.axvline(x=X_next, ls='--', c='k', lw=1)\n",
    "    if show_legend:\n",
    "        plt.legend()\n",
    "'''\n",
    "def plot_acquisition_2d(X, Y, X_next, show_legend=False):\n",
    "    plt.plot(X, Y, 'r-', lw=1, label='Acquisition function')\n",
    "    plt.axvline(x=X_next, ls='--', c='k', lw=1, label='Next sampling location')\n",
    "    if show_legend:\n",
    "        plt.legend()   '''\n",
    "def plot_acquisition_2d(X, plot_means, X_next, show_legend=False):\n",
    "    #print('x',X[0].shape)\n",
    "    #print('mean',plot_means.shape)\n",
    "    \n",
    "    X,Y=grid=np.meshgrid(X[:, 0],X[:, 1])\n",
    "    x_test=np.vstack([X.flatten(),Y.flatten()]).T\n",
    "    #plt.contour(X, Y, plot_means.reshape(X.shape[0], X.shape[0]),levels=700, linewidths=0.5, colors='k')\n",
    "    plt.contourf(X, Y, plot_means.reshape(X.shape[0], X.shape[0]), levels=700, cmap=\"RdBu_r\")\n",
    "    #plt.clabel(CS, inline=1, fontsize=10)\n",
    "    #print(X_next)\n",
    "    plt.plot(X_next.T[:, 0], X_next.T[:, 1], 'co', mew=3, label='Acquisition Fn - Next Sample')\n",
    "    cntr1 = plt.contourf(X, Y, plot_means.reshape(X.shape[0], X.shape[0]), levels=10, cmap=\"RdBu_r\")\n",
    "    if show_legend:\n",
    "        plt.colorbar(cntr1)\n",
    "        plt.legend()\n",
    "        \n",
    "def plot_acquisition(X, Y, X_next, show_legend=False):\n",
    "    plt.plot(X, Y, 'r-', lw=1, label='Acquisition function')\n",
    "    plt.axvline(x=X_next, ls='--', c='k', lw=1, label='Next sampling location')\n",
    "    if show_legend:\n",
    "        plt.legend()    \n",
    "        \n",
    "def plot_convergence(X_sample, Y_sample, n_init=2):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "\n",
    "    x = X_sample[n_init:].ravel()\n",
    "    y = Y_sample[n_init:].ravel()\n",
    "    r = range(1, len(x)+1)\n",
    "    \n",
    "    x_neighbor_dist = [np.abs(a-b) for a, b in zip(x, x[1:])]\n",
    "    y_max_watermark = np.maximum.accumulate(y)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(r[1:], x_neighbor_dist, 'bo-')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title('Distance between consecutive x\\'s')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(r, y_max_watermark, 'ro-')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Best Y')\n",
    "    plt.title('Value of best selected sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "import GPy\n",
    "'''\n",
    "    Bayesian Optimization Driver Methods\n",
    "    Kernel, GPR posterios update\n",
    "'''\n",
    "def bayOptEI(n_iter, X, X_init, Y_init, f, kernel, mf,\\\n",
    "             mpi=False, lr=0.4,opt=False,bounds=np.array([[0.1, 1]]),plotPlots=False):\n",
    "    X_sample = X_init\n",
    "    Y_sample = Y_init\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Update Gaussian process with existing samples\n",
    "        if(mf):\n",
    "            gpr = GPy.models.GPRegression(X_sample,Y_sample,kernel=kernel, mean_function=mf)\n",
    "        else:\n",
    "            gpr = GPy.models.GPRegression(X_sample,Y_sample,kernel=kernel)\n",
    "        \n",
    "\n",
    "        # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "        X_next = propose_location(expected_improvement, X_sample, Y_sample, gpr, bounds)[0, 0]\n",
    "    \n",
    "        # Obtain next noisy sample from the objective function\n",
    "        #Y_next = Deep_cont(testPolicy, max_steps)\n",
    "        #Y_next = f(testPolicy, agentTrainer, lr=X[])\n",
    "        Y_next = f(gamma=X_next, live=False, lr=lr, best_samples=Y_sample.flatten())\n",
    "        \n",
    "        if(plotPlots):\n",
    "            # Plot samples, surrogate function, noise-free objective and next sampling location\n",
    "            if(i%2 == 0):\n",
    "            #if(True):\n",
    "                plt.subplot(n_iter, 2, i + 1)\n",
    "                #plt.subplot(n_iter, 2, i+1)\n",
    "                plot_app(gpr, X, X_sample, Y_sample, X_next, noisy_label='Gamma', show_legend=True)\n",
    "                plt.title(f'Iteration {i+1}')\n",
    "\n",
    "                plt.subplot(n_iter, 2, i + 2)\n",
    "                #plt.subplot(n_iter, 2, i+2)\n",
    "                plot_acquisition(X, expected_improvement(X, X_sample, Y_sample, gpr, mpi=mpi), X_next, show_legend=True)\n",
    "    \n",
    "        # Add sample to previous samples\n",
    "        X_sample = np.vstack((X_sample, X_next))\n",
    "        Y_sample = np.vstack((Y_sample, Y_next))\n",
    "    #plt.tight_layout()\n",
    "    #gpr.save_model('gprmodel1D', compress=True, save_data=True)\n",
    "    return X_sample, Y_sample\n",
    "\n",
    "def bayOptEI_2d(n_iter, X, X_init, Y_init, f, kernel, mpi=False,\\\n",
    "             mf=False, lr=0.4,bounds =np.array([[0.1, 0.9], [0.01, 0.2]]), plotPlots=False):\n",
    "    noise = 0.\n",
    "    X_sample = X_init\n",
    "    Y_sample = Y_init\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        if(mf):\n",
    "            gpr = GPy.models.GPRegression(X_sample,Y_sample,kernel=kernel, mean_function=mf)\n",
    "        else:\n",
    "            gpr = GPy.models.GPRegression(X_sample,Y_sample,kernel=kernel)\n",
    "            \n",
    "        # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "        X_next = propose_location_2d(expected_improvement_2d, X_sample, Y_sample, gpr, bounds)\n",
    "        # Obtain next noisy sample from the objective function\n",
    "        #Y_next = Deep_cont(testPolicy, max_steps)\n",
    "        #Y_next = f(testPolicy, agentTrainer, lr=X[])\n",
    "        print(X_next)\n",
    "        Y_next = f(gamma=X_next[0][0], live=False, lr=X_next[1][0], best_samples=Y_sample.flatten())\n",
    "        if(plotPlots):\n",
    "            # Plot samples, surrogate function, noise-free objective and next sampling location\n",
    "            if(i%3 == 0):\n",
    "                #plt.subplot(n_iter, 2, 2 * i + 1)\n",
    "                #plt.subplot(n_iter, 2, i+1)\n",
    "                plt.subplot(n_iter, 3, i+1)\n",
    "                plot_contour(gpr, (X[:, 0], X[:, 1]), X_sample, Y_sample, X_next, \\\n",
    "                             noisy_label='Gamma', show_legend=i==0)\n",
    "                plt.title(f'Mean Iteration {i+1}')\n",
    "                \n",
    "                plt.subplot(n_iter, 3, i + 2)\n",
    "                plot_contour_var(gpr, (X[:, 0], X[:, 1]), X_sample, Y_sample, X_next, \\\n",
    "                             noisy_label='Gamma', showLegend=i==0)\n",
    "                plt.title(f'Variance Iteration {i+1}')\n",
    "                \n",
    "                plt.subplot(n_iter, 3, i + 3)\n",
    "                plot_acquisition_2d(X, expected_improvement_2d(X, X_sample, Y_sample, gpr, mpi=mpi,plot=True), \\\n",
    "                                    X_next, show_legend=i==0)\n",
    "                plt.title(f'Acquisition {i+1}')\n",
    "                #plot_acquisition(X, expected_improvement(X, X_sample, Y_sample, gpr), X_next, show_legend=i==0)\n",
    "    \n",
    "        # Add sample to previous samples\n",
    "        X_sample = np.vstack((X_sample, X_next.T))\n",
    "        Y_sample = np.vstack((Y_sample, Y_next))\n",
    "    #plt.tight_layout()\n",
    "    return X_sample, Y_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPML_RL_GPy",
   "language": "python",
   "name": "npml_rl_gpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
